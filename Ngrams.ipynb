{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import operator\n",
    "import math\n",
    "import re\n",
    "\n",
    "with open(r'C:\\Users\\Saim\\Downloads\\shakespeare.txt', encoding=\"utf8\") as f:\n",
    "    file = f.read()\n",
    "\n",
    "def prep_tokens(text):\n",
    "    lower_words = text.lower()\n",
    "    remove_special_char = re.sub(r'[-.,;:!?\\'\\d+]+', \"\", lower_words)\n",
    "    tokens = re.findall(r'\\w+', remove_special_char)\n",
    "\n",
    "    vocabulary = list(tokens)\n",
    "    vocab_size = len((vocabulary))\n",
    "    \n",
    "    return tokens, vocab_size\n",
    "\n",
    "tokens, V = prep_tokens(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(tokens):\n",
    "    unigrams = list(ngrams(tokens, 1, \n",
    "                           pad_left=True, \n",
    "                           pad_right=True, \n",
    "                           left_pad_symbol='<s>', \n",
    "                           right_pad_symbol='</s>')\n",
    "                   )\n",
    "    \n",
    "    unigram_count = Counter(unigrams)\n",
    "    \n",
    "    unigram_prob = {}\n",
    "    \n",
    "    for key in unigram_count.keys():\n",
    "        unigram_prob[key] = math.log(float(unigram_count[key]) / V, 2)\n",
    "    #print(unigram_prob)\n",
    "    return unigram_prob, unigram_count\n",
    "\n",
    "uni_prob, uni_count = unigram_prob(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(tokens):\n",
    "    bigrams = list(ngrams(tokens, 2,\n",
    "                          pad_left=True, \n",
    "                          pad_right=True, \n",
    "                          left_pad_symbol='<s>', \n",
    "                          right_pad_symbol='</s>')\n",
    "                  )\n",
    "    bigram_count = Counter(bigrams)\n",
    "    \n",
    "    bigram_prob = {}\n",
    "  \n",
    "    for word in bigram_count:\n",
    "        bigram_prob[tuple(word)] = math.log(float(bigram_count[word]+1) / ((uni_count[word[0]]) + V*1),2)\n",
    "    \n",
    "    return bigram_prob, bigram_count\n",
    "\n",
    "bi_prob, bi_count = bigram_prob(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_prob(tokens):\n",
    "    trigrams = list(ngrams(tokens, 3, \n",
    "                           pad_left=True, \n",
    "                           pad_right=True, \n",
    "                           left_pad_symbol='<s>', \n",
    "                           right_pad_symbol='</s>')\n",
    "                   )\n",
    "    \n",
    "    trigram_count = Counter(trigrams)\n",
    "    \n",
    "    trigram_prob = {}\n",
    "        \n",
    "    for word in trigram_count:\n",
    "        trigram_prob[tuple(word)] = math.log(float(trigram_count[word] +1)/ (V*1+bi_count[(word[0], word[1])]), 2)\n",
    "    #print(trigram_prob)\n",
    "    return trigram_prob\n",
    "\n",
    "tri_prob = trigram_prob(tokens)\n",
    "#print(tri_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_matrix(uni_prob, bi_prob, tri_prob):\n",
    "    \n",
    "    df_1 = pd.DataFrame(uni_prob.items(), columns=['Word', 'Probability'])\n",
    "    df_2 = pd.DataFrame(bi_prob.items(), columns=['Word', 'Probability'])\n",
    "    df_3 = pd.DataFrame(tri_prob.items(), columns=['Word', 'Probability'])\n",
    "    \n",
    "    first_join = df_1.join(df_2, lsuffix='1')\n",
    "    final_join = first_join.join(df_3, lsuffix='2')\n",
    "    \n",
    "    return final_join\n",
    "\n",
    "matrix = prob_matrix(uni_prob, bi_prob, tri_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolated_prob(prob_matrix):\n",
    "    \n",
    "    lambda_1 = 1/3\n",
    "    lambda_2 = 1/3\n",
    "    lambda_3 = 1/3\n",
    "    \n",
    "    #print(prob_matrix)\n",
    "    interpolated_probability = lambda_1*prob_matrix[\"Probability1\"] + lambda_2*prob_matrix[\"Probability2\"] + lambda_3*prob_matrix[\"Probability\"]\n",
    "    mean_prob = interpolated_probability.mean()\n",
    "    df = pd.concat([prob_matrix, interpolated_probability.rename('Interpolated')], axis=1)\n",
    "    \n",
    "    return df, mean_prob\n",
    "inter_prob, mean = interpolated_prob(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda set 4  contains the lambda parameters that results in the highest log likelihood and should be used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda set 1': -14.549595502589842,\n",
       " 'lambda set 2': -14.508932831612574,\n",
       " 'lambda set 3': -14.468270160635303,\n",
       " 'lambda set 4': -14.427607489658033}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimize_lambda(prob_matrix):\n",
    "    \n",
    "    lambda_1_set_1, lambda_2_set_1,lambda_3_set_1 = 0.2, 0.2, 0.6\n",
    "\n",
    "    lambda_1_set_2, lambda_2_set_2, lambda_3_set_2 = 0.3, 0.3, 0.4\n",
    "    \n",
    "    lambda_1_set_3, lambda_2_set_3, lambda_3_set_3 = 0.4, 0.4, 0.2\n",
    "    \n",
    "    lambda_1_set_4, lambda_2_set_4, lambda_3_set_4 = 0.5, 0.5, 0.0\n",
    "\n",
    "    mean_dict = {}\n",
    "    \n",
    "    ip1 = lambda_1_set_1*prob_matrix[\"Probability1\"] + lambda_2_set_1*prob_matrix[\"Probability2\"] + lambda_3_set_1*prob_matrix[\"Probability\"]\n",
    "    mean_dict[\"lambda set 1\"]=ip1.mean()\n",
    "    \n",
    "    ip2 = lambda_1_set_2*prob_matrix[\"Probability1\"] + lambda_2_set_2*prob_matrix[\"Probability2\"] + lambda_3_set_2*prob_matrix[\"Probability\"]\n",
    "    mean_dict[\"lambda set 2\"]=ip2.mean()\n",
    "    \n",
    "    ip3 = lambda_1_set_3*prob_matrix[\"Probability1\"] + lambda_2_set_3*prob_matrix[\"Probability2\"] + lambda_3_set_3*prob_matrix[\"Probability\"]\n",
    "    mean_dict[\"lambda set 3\"]=ip3.mean()\n",
    "    \n",
    "    ip4 = lambda_1_set_4*prob_matrix[\"Probability1\"] + lambda_2_set_4*prob_matrix[\"Probability2\"] + lambda_3_set_4*prob_matrix[\"Probability\"]\n",
    "    mean_dict[\"lambda set 4\"]=ip4.mean()\n",
    "    \n",
    "    \n",
    "    print(max(mean_dict.items(), key=operator.itemgetter(1))[0],\" contains the lambda parameters that results in the highest log likelihood and should be used\")\n",
    "    return mean_dict\n",
    "optimize_lambda(inter_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_with_bigram(text: str, bi_prob):\n",
    "\n",
    "    words, size = prep_tokens(text)\n",
    "    previous_word = words[-1:]\n",
    "\n",
    "    sorted_bigram = dict(sorted(bi_prob.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    for bigram in sorted_bigram.keys():\n",
    "        if bigram[0:1] == tuple(previous_word):\n",
    "            previous_word.append(bigram[1])\n",
    "        else:\n",
    "            print(\"No match\")\n",
    "            break\n",
    "    sentence = text + \" \" + previous_word[-1]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_with_trigram(text: str, tri_prob):\n",
    "\n",
    "    word_1, size = prep_tokens(text)\n",
    "    last_2 = word_1[-2:]\n",
    "\n",
    "    sorted_trigram = dict(sorted(tri_prob.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    for trigram in sorted_trigram.keys():\n",
    "        if trigram[0:2] == tuple(last_2):\n",
    "            last_2.append(trigram[2])\n",
    "        else:\n",
    "            print(\"No match\")\n",
    "            #break\n",
    "    sentence = text + \" \" + last_2[-1]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gen_text_with_trigram(streng, model):\n",
    "        \n",
    "        max_words = 20\n",
    "        count = 0\n",
    "        \n",
    "        while  count != max_words:\n",
    "            s = next_word_with_trigram(streng, model)\n",
    "            streng = s\n",
    "            count = count +1\n",
    "        return streng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gen_text_with_bigram(streng, model):\n",
    "        \n",
    "        max_words = 30\n",
    "        count = 0\n",
    "        \n",
    "        while  count != max_words:\n",
    "            s = next_word_with_bigram(streng, model)\n",
    "            streng = s\n",
    "            count = count +1\n",
    "        return streng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
